## Test Post - Policy Gradient Overview




## Overview

The goal of this document is to give a solid summary of vanilla policy gradient (i.e. the REINFORCE algorithm). Policy gradient is the foundation of many common RL algorithms such as PPO, TRPO, DPG, DDPG, etc. In addition, actor-critic algorithms stem from PG, as the actor learns a parameterized policy while the critic learns a parameterized value function.


This overview will contain:
- explanation of the key terms
- an overview of the math and algorithm behind PG
- implementation of the algorithm in tensorflow
- implementation of use of tensorboard for tracking progress of algorithm

---

### KEY TERMS: 
    
$\pi_\theta(a|s)$ : probability of taking action $a$ given state $s$ with $\theta$ representing the parameters (or weights in the network) of our policy

$\theta$ : parameters or weights being used in the network - we perform gradient ascent on these params

$\alpha$ : learning rate (alpha)

$\tau$ : trajectory, often called a rollout  $\tau = S_0, A_0, R_1, S_1, A_1,...,T_{T-1},A_{T-1}, R_T$

$r(\tau)$ : reward for trajectory $\tau$

$\gamma$ : discount rate (gamma)

$G_t$ : return (cumulative discounted reward) following time t - takes advantage of the fact that rewards in the past do not contribute to future decisions
$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$

```python
class Policy_MLP(object):
    def __init__(self, nb_actions, name='policy'):
        self.name = name
        self.nb_actions = nb_actions
        
    @property
    def vars(self):
        return tf.get_collection(
            tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)
    
    @property
    def trainable_var(self):
        return tf.get_collection(
            tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)
        
    def __call__(self, obs):
        with tf.variable_scope(self.name):
            x = tf.layers.dense(
                obs, 
                use_bias=False, 
                units=8, 
                activation=tf.nn.relu)
            out = tf.layers.dense(
                x, 
                use_bias=False, 
                units=self.nb_actions, 
                activation=tf.nn.softmax)
            return out
```
