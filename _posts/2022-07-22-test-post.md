## Test Post - Policy Gradient Overview




## Overview

The goal of this document is to give a solid summary of vanilla policy gradient (i.e. the REINFORCE algorithm). Policy gradient is the foundation of many common RL algorithms such as PPO, TRPO, DPG, DDPG, etc. In addition, actor-critic algorithms stem from PG, as the actor learns a parameterized policy while the critic learns a parameterized value function.


This overview will contain:
- explanation of the key terms
- an overview of the math and algorithm behind PG
- implementation of the algorithm in tensorflow
- implementation of use of tensorboard for tracking progress of algorithm

---

### KEY TERMS: 

$$L = \sum x_i$$

$$ {X}_{0} $$ (works)
$$ X_0 $$ (works)

\begin{equation}
\begin{aligned}
  {X}_{0} (does not always work)
  {X}\_{0} (works)
  X_0 (works)
  \hat{a}_{b} (does not always work)
  \hat{a}_b (works)
  \hat{a}_{b+c} (works)
\end{aligned}
\end{equation}



\\begin{equation}
  {X}_{0} 
  {X}\_{0} 
  X_0 
  \hat{a}_{b}
  \hat{a}_b 
  \hat{a}_{b+c} 
\\end{equation}


$\pi_\theta(a|s)$ : probability of taking action $a$ given state $s$ with $\theta$ representing the parameters (or weights in the network) of our policy

$\theta$ : parameters or weights being used in the network - we perform gradient ascent on these params

$\alpha$ : learning rate (alpha)

$\tau$ : trajectory, often called a rollout  $\tau = S_0, A_0, R_1, S_1, A_1,...,T_{T-1},A_{T-1}, R_T$

$r(\tau)$ : reward for trajectory $\tau$

$\gamma$ : discount rate (gamma)

$G_t$ : return (cumulative discounted reward) following time t - takes advantage of the fact that rewards in the past do not contribute to future decisions
$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$

```python
class Policy_MLP(object):
    def __init__(self, nb_actions, name='policy'):
        self.name = name
        self.nb_actions = nb_actions
        
    @property
    def vars(self):
        return tf.get_collection(
            tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)
    
    @property
    def trainable_var(self):
        return tf.get_collection(
            tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name)
        
    def __call__(self, obs):
        with tf.variable_scope(self.name):
            x = tf.layers.dense(
                obs, 
                use_bias=False, 
                units=8, 
                activation=tf.nn.relu)
            out = tf.layers.dense(
                x, 
                use_bias=False, 
                units=self.nb_actions, 
                activation=tf.nn.softmax)
            return out
```
